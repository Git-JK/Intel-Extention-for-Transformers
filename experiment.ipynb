{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2bd28e-0882-4cc4-8d3c-5b0d17dec65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:13:19.362597Z",
     "iopub.status.busy": "2024-05-27T11:13:19.362280Z",
     "iopub.status.idle": "2024-05-27T11:14:23.862801Z",
     "shell.execute_reply": "2024-05-27T11:14:23.862225Z",
     "shell.execute_reply.started": "2024-05-27T11:13:19.362569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'chatglm3-6b'...\n",
      "remote: Enumerating objects: 140, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 140 (delta 8), reused 1 (delta 0), pack-reused 122\u001b[K\n",
      "接收对象中: 100% (140/140), 61.16 KiB | 30.58 MiB/s, 完成.\n",
      "处理 delta 中: 100% (60/60), 完成.\n",
      "过滤内容: 100% (15/15), 23.26 GiB | 372.19 MiB/s, 完成.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43adf093-cd8b-4837-a112-baf91dda105c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:14:50.157850Z",
     "iopub.status.busy": "2024-05-27T11:14:50.157501Z",
     "iopub.status.idle": "2024-05-27T11:16:15.572108Z",
     "shell.execute_reply": "2024-05-27T11:16:15.571550Z",
     "shell.execute_reply.started": "2024-05-27T11:14:50.157824Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'bge-base-zh-v1.5'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 30 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "接收对象中: 100% (30/30), 168.35 KiB | 28.06 MiB/s, 完成.\n",
      "处理 delta 中: 100% (5/5), 完成.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://www.modelscope.cn/AI-ModelScope/bge-base-zh-v1.5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b794c89-c51a-4095-89a9-1026b52f1c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:19:10.463181Z",
     "iopub.status.busy": "2024-05-27T11:19:10.462601Z",
     "iopub.status.idle": "2024-05-27T11:27:11.205443Z",
     "shell.execute_reply": "2024-05-27T11:27:11.204938Z",
     "shell.execute_reply.started": "2024-05-27T11:19:10.463157Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:19:10,465 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: ./bge-base-zh-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create retrieval plugin instance...\n",
      "plugin parameters:  {'embedding_model': './bge-base-zh-v1.5', 'input_path': './sample.jsonl'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:19:11,856 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
      "2024-05-27 19:19:11,859 - root - INFO - The parsing for the uploaded files is finished.\n",
      "2024-05-27 19:19:11,859 - root - INFO - The format of parsed documents is transferred.\n",
      "2024-05-27 19:19:11,865 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca3338002374f6e8c05f9930efc18ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:19:13,750 - root - INFO - The retriever is successfully built.\n",
      "2024-05-27 19:19:13,854 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting eos_token is not supported, use the default one.\n",
      "2024-05-27 19:19:13,854 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting pad_token is not supported, use the default one.\n",
      "2024-05-27 19:19:13,854 - transformers_modules.chatglm3_6b.tokenization_chatglm - WARNING - Setting unk_token is not supported, use the default one.\n",
      "2024-05-27 19:19:13 [INFO] Applying Weight Only Quantization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ./chatglm3-6b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4490c6e485e74c17828e85f6543e58b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:19:44 [INFO] Start auto tuning.\n",
      "2024-05-27 19:19:44 [INFO] Quantize model without tuning!\n",
      "2024-05-27 19:19:44 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2024-05-27 19:19:44 [INFO] Adaptor has 5 recipes.\n",
      "2024-05-27 19:19:44 [INFO] 0 recipes specified by user.\n",
      "2024-05-27 19:19:44 [INFO] 3 recipes require future tuning.\n",
      "2024-05-27 19:19:44 [INFO] *** Initialize auto tuning\n",
      "2024-05-27 19:19:44 [INFO] {\n",
      "2024-05-27 19:19:44 [INFO]     'PostTrainingQuantConfig': {\n",
      "2024-05-27 19:19:44 [INFO]         'AccuracyCriterion': {\n",
      "2024-05-27 19:19:44 [INFO]             'criterion': 'relative',\n",
      "2024-05-27 19:19:44 [INFO]             'higher_is_better': True,\n",
      "2024-05-27 19:19:44 [INFO]             'tolerable_loss': 0.01,\n",
      "2024-05-27 19:19:44 [INFO]             'absolute': None,\n",
      "2024-05-27 19:19:44 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f54e54c8940>>,\n",
      "2024-05-27 19:19:44 [INFO]             'relative': 0.01\n",
      "2024-05-27 19:19:44 [INFO]         },\n",
      "2024-05-27 19:19:44 [INFO]         'approach': 'post_training_weight_only',\n",
      "2024-05-27 19:19:44 [INFO]         'backend': 'default',\n",
      "2024-05-27 19:19:44 [INFO]         'calibration_sampling_size': [\n",
      "2024-05-27 19:19:44 [INFO]             100\n",
      "2024-05-27 19:19:44 [INFO]         ],\n",
      "2024-05-27 19:19:44 [INFO]         'device': 'cpu',\n",
      "2024-05-27 19:19:44 [INFO]         'diagnosis': False,\n",
      "2024-05-27 19:19:44 [INFO]         'domain': 'auto',\n",
      "2024-05-27 19:19:44 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2024-05-27 19:19:44 [INFO]         'excluded_precisions': [\n",
      "2024-05-27 19:19:44 [INFO]         ],\n",
      "2024-05-27 19:19:44 [INFO]         'framework': 'pytorch_fx',\n",
      "2024-05-27 19:19:44 [INFO]         'inputs': [\n",
      "2024-05-27 19:19:44 [INFO]         ],\n",
      "2024-05-27 19:19:44 [INFO]         'model_name': '',\n",
      "2024-05-27 19:19:44 [INFO]         'ni_workload_name': 'quantization',\n",
      "2024-05-27 19:19:44 [INFO]         'op_name_dict': {\n",
      "2024-05-27 19:19:44 [INFO]             '.*lm_head': {\n",
      "2024-05-27 19:19:44 [INFO]                 'weight': {\n",
      "2024-05-27 19:19:44 [INFO]                     'dtype': [\n",
      "2024-05-27 19:19:44 [INFO]                         'fp32'\n",
      "2024-05-27 19:19:44 [INFO]                     ]\n",
      "2024-05-27 19:19:44 [INFO]                 }\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             '.*output_layer': {\n",
      "2024-05-27 19:19:44 [INFO]                 'weight': {\n",
      "2024-05-27 19:19:44 [INFO]                     'dtype': [\n",
      "2024-05-27 19:19:44 [INFO]                         'fp32'\n",
      "2024-05-27 19:19:44 [INFO]                     ]\n",
      "2024-05-27 19:19:44 [INFO]                 }\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             '.*embed_out': {\n",
      "2024-05-27 19:19:44 [INFO]                 'weight': {\n",
      "2024-05-27 19:19:44 [INFO]                     'dtype': [\n",
      "2024-05-27 19:19:44 [INFO]                         'fp32'\n",
      "2024-05-27 19:19:44 [INFO]                     ]\n",
      "2024-05-27 19:19:44 [INFO]                 }\n",
      "2024-05-27 19:19:44 [INFO]             }\n",
      "2024-05-27 19:19:44 [INFO]         },\n",
      "2024-05-27 19:19:44 [INFO]         'op_type_dict': {\n",
      "2024-05-27 19:19:44 [INFO]             '.*': {\n",
      "2024-05-27 19:19:44 [INFO]                 'weight': {\n",
      "2024-05-27 19:19:44 [INFO]                     'bits': [\n",
      "2024-05-27 19:19:44 [INFO]                         4\n",
      "2024-05-27 19:19:44 [INFO]                     ],\n",
      "2024-05-27 19:19:44 [INFO]                     'dtype': [\n",
      "2024-05-27 19:19:44 [INFO]                         'int4'\n",
      "2024-05-27 19:19:44 [INFO]                     ],\n",
      "2024-05-27 19:19:44 [INFO]                     'group_size': [\n",
      "2024-05-27 19:19:44 [INFO]                         32\n",
      "2024-05-27 19:19:44 [INFO]                     ],\n",
      "2024-05-27 19:19:44 [INFO]                     'scheme': [\n",
      "2024-05-27 19:19:44 [INFO]                         'sym'\n",
      "2024-05-27 19:19:44 [INFO]                     ],\n",
      "2024-05-27 19:19:44 [INFO]                     'algorithm': [\n",
      "2024-05-27 19:19:44 [INFO]                         'RTN'\n",
      "2024-05-27 19:19:44 [INFO]                     ]\n",
      "2024-05-27 19:19:44 [INFO]                 }\n",
      "2024-05-27 19:19:44 [INFO]             }\n",
      "2024-05-27 19:19:44 [INFO]         },\n",
      "2024-05-27 19:19:44 [INFO]         'outputs': [\n",
      "2024-05-27 19:19:44 [INFO]         ],\n",
      "2024-05-27 19:19:44 [INFO]         'quant_format': 'default',\n",
      "2024-05-27 19:19:44 [INFO]         'quant_level': 'auto',\n",
      "2024-05-27 19:19:44 [INFO]         'recipes': {\n",
      "2024-05-27 19:19:44 [INFO]             'smooth_quant': False,\n",
      "2024-05-27 19:19:44 [INFO]             'smooth_quant_args': {\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             'layer_wise_quant': False,\n",
      "2024-05-27 19:19:44 [INFO]             'layer_wise_quant_args': {\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             'fast_bias_correction': False,\n",
      "2024-05-27 19:19:44 [INFO]             'weight_correction': False,\n",
      "2024-05-27 19:19:44 [INFO]             'gemm_to_matmul': True,\n",
      "2024-05-27 19:19:44 [INFO]             'graph_optimization_level': None,\n",
      "2024-05-27 19:19:44 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2024-05-27 19:19:44 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2024-05-27 19:19:44 [INFO]             'pre_post_process_quantization': True,\n",
      "2024-05-27 19:19:44 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2024-05-27 19:19:44 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2024-05-27 19:19:44 [INFO]             ],\n",
      "2024-05-27 19:19:44 [INFO]             'dedicated_qdq_pair': False,\n",
      "2024-05-27 19:19:44 [INFO]             'rtn_args': {\n",
      "2024-05-27 19:19:44 [INFO]                 'enable_full_range': True,\n",
      "2024-05-27 19:19:44 [INFO]                 'enable_mse_search': False\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             'awq_args': {\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             'gptq_args': {\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             'teq_args': {\n",
      "2024-05-27 19:19:44 [INFO]             },\n",
      "2024-05-27 19:19:44 [INFO]             'autoround_args': {\n",
      "2024-05-27 19:19:44 [INFO]             }\n",
      "2024-05-27 19:19:44 [INFO]         },\n",
      "2024-05-27 19:19:44 [INFO]         'reduce_range': None,\n",
      "2024-05-27 19:19:44 [INFO]         'TuningCriterion': {\n",
      "2024-05-27 19:19:44 [INFO]             'max_trials': 100,\n",
      "2024-05-27 19:19:44 [INFO]             'objective': [\n",
      "2024-05-27 19:19:44 [INFO]                 'performance'\n",
      "2024-05-27 19:19:44 [INFO]             ],\n",
      "2024-05-27 19:19:44 [INFO]             'strategy': 'basic',\n",
      "2024-05-27 19:19:44 [INFO]             'strategy_kwargs': None,\n",
      "2024-05-27 19:19:44 [INFO]             'timeout': 0\n",
      "2024-05-27 19:19:44 [INFO]         },\n",
      "2024-05-27 19:19:44 [INFO]         'use_bf16': True\n",
      "2024-05-27 19:19:44 [INFO]     }\n",
      "2024-05-27 19:19:44 [INFO] }\n",
      "2024-05-27 19:19:44 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2024-05-27 19:19:44 [INFO] Pass query framework capability elapsed time: 2.72 ms\n",
      "2024-05-27 19:19:44 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2024-05-27 19:19:44 [INFO] Quantize the model with default config.\n",
      "2024-05-27 19:19:44 [INFO] All algorithms to do: {'RTN'}\n",
      "2024-05-27 19:19:44 [INFO] quantizing with the round-to-nearest algorithm\n",
      "2024-05-27 19:22:08 [INFO] |******Mixed Precision Statistics******|\n",
      "2024-05-27 19:22:08 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-27 19:22:08 [INFO] | Op Type | Total |  A32W4G32 |  FP32  |\n",
      "2024-05-27 19:22:08 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-27 19:22:08 [INFO] |  Linear |  113  |    112    |   1    |\n",
      "2024-05-27 19:22:08 [INFO] +---------+-------+-----------+--------+\n",
      "2024-05-27 19:22:08 [INFO] Pass quantize model elapsed time: 143153.12 ms\n",
      "2024-05-27 19:22:08 [INFO] Save tuning history to /mnt/workspace/nc_workspace/2024-05-27_19-18-32/./history.snapshot.\n",
      "2024-05-27 19:22:08 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2024-05-27 19:22:08 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2024-05-27 19:22:08 [INFO] Save deploy yaml to /mnt/workspace/nc_workspace/2024-05-27_19-18-32/deploy.yaml\n",
      "2024-05-27 19:27:11 [INFO] WeightOnlyQuant done.\n",
      "2024-05-27 19:27:11,203 - root - INFO - Optimized Model loaded.\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig \n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot \n",
    "from intel_extension_for_transformers.neural_chat import plugins \n",
    "from intel_extension_for_transformers.transformers import RtnConfig \n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args['embedding_model'] = \"./bge-base-zh-v1.5\" \n",
    "plugins.retrieval.args[\"input_path\"]=\"./sample.jsonl\" \n",
    "config = PipelineConfig(model_name_or_path='./chatglm3-6b', plugins=plugins, optimization_config=RtnConfig(compute_dtype=\"int8\", weight_dtype=\"int4_fullrange\")) \n",
    "chatbot = build_chatbot(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f03d46-db69-4379-b955-552356643710",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-05-27T11:30:30.198742Z",
     "iopub.status.busy": "2024-05-27T11:30:30.198396Z",
     "iopub.status.idle": "2024-05-27T11:30:53.568279Z",
     "shell.execute_reply": "2024-05-27T11:30:53.567770Z",
     "shell.execute_reply.started": "2024-05-27T11:30:30.198720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关于cnvrg.io网站的创建者信息，我目前无法提供确切的信息。请您尝试联系相关客服或查询网站源代码以获取更多信息。\n"
     ]
    }
   ],
   "source": [
    "plugins.retrieval.enable=False # disable retrieval \n",
    "response = chatbot.predict(query=\"cnvrg.io网站是由谁创建的？ 网站是由谁创建的？ 网站是由谁创建的？ 网站是由谁创建的？ \") \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358e58ca-b9fc-4f62-a700-f06544ab3587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-27T11:30:58.305487Z",
     "iopub.status.busy": "2024-05-27T11:30:58.305141Z",
     "iopub.status.idle": "2024-05-27T11:31:57.262662Z",
     "shell.execute_reply": "2024-05-27T11:31:57.262123Z",
     "shell.execute_reply.started": "2024-05-27T11:30:58.305466Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d757cecd1dc426ab6f332ccda22b28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:31:37,264 - chromadb.segment.impl.vector.local_persistent_hnsw - WARNING - Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n",
      "2024-05-27 19:31:37,267 - root - INFO - Chat with QA Agent.\n",
      "/opt/conda/envs/itrex/lib/python3.10/site-packages/torch/amp/autocast_mode.py:267: UserWarning: In CPU autocast, but the target dtype is not supported. Disabling autocast.\n",
      "CPU Autocast only supports dtype of torch.bfloat16, torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该网站是由Yochay Ettun和Leah Forkosh Kolben创建的。\n"
     ]
    }
   ],
   "source": [
    "plugins.retrieval.enable=True # enable retrieval \n",
    "response = chatbot.predict(query=\"cnvrg.io网站是由谁创建的？ 网站是由谁创建的？ 网站是由谁创建的？ 网站是由谁创建的？ \") \n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
